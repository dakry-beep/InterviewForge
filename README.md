# InterviewForge

**Forge your qualitative interviews into scientific transcripts**

Automatisierte Audio-Transkription mit Sprecherdiarisation im wissenschaftlichen Kruse-Format

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![OpenAI Whisper](https://img.shields.io/badge/OpenAI-Whisper-green.svg)](https://openai.com/research/whisper)
[![Pyannote Audio](https://img.shields.io/badge/Pyannote-Audio-orange.svg)](https://github.com/pyannote/pyannote-audio)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## Features

- âœ… **Grafische BenutzeroberflÃ¤che (GUI)** - einfach zu bedienen
- âœ… **Automatische Transkription** mit OpenAI Whisper (API oder lokal)
- âœ… **Datenschutz-Modus** mit lokalem Whisper (kein API-Key nÃ¶tig)
- âœ… **Sprecherdiarisation** mit Pyannote Audio 3.1
- âœ… **Wissenschaftliches Format** (Kruse-Notation)
- âœ… **Audio-Optimierung** (FFmpeg)
- âœ… **GPU-beschleunigt** (CUDA-Support)
- âœ… **Batch-Verarbeitung** mehrerer Dateien
- âœ… **Konfigurierbar** via YAML

---

## Beispiel-Output

```
================================================================================
Transkript: interview_01.wav
Datum: 04.11.2025
Format: Kruse-Notation (OpenAI Whisper + Pyannote)
================================================================================

LEGENDE:
  I: SPEAKER_00 (Interviewer)
  P1: SPEAKER_01 (Person 1)

SYMBOLE:
  (.): Kurze Pause
  (..): Mittlere Pause
  (3s): Lange Pause
  ((lacht)): Lachen
  (unv.): UnverstÃ¤ndlich

================================================================================

  1 [00:01] I: Wie wÃ¼rden Sie Ihre Erfahrungen beschreiben?
  2
  3 [00:05] P1: Also (..) ich kann sagen, dass es _sehr_ interessant war.
  4     Besonders die ersten Wochen (.) waren herausfordernd.
  5
  6 [00:15] I: KÃ¶nnen Sie das genauer erlÃ¤utern?
```

---

## Installation

### 1. Voraussetzungen

- Python 3.10 oder hÃ¶her
- FFmpeg
- OpenAI API Key
- Hugging Face Account + Token
- (Optional) NVIDIA GPU mit CUDA fÃ¼r beschleunigte Diarisation

### 2. Repository klonen

```bash
git clone https://github.com/yourusername/InterviewForge.git
cd InterviewForge
```

### 3. Virtual Environment erstellen

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows
```

### 4. Dependencies installieren

**FÃ¼r API-Modus (empfohlen fÃ¼r beste QualitÃ¤t):**
```bash
pip install -r requirements.txt
```

**FÃ¼r Lokal-Modus (Datenschutz, ohne OpenAI API):**
```bash
pip install -r requirements-local.txt
```

**FÃ¼r GUI (grafische OberflÃ¤che):**

Die GUI nutzt `tkinter`, das bei den meisten Python-Installationen bereits enthalten ist.

**Falls tkinter fehlt:**

**Ubuntu/Debian:**
```bash
sudo apt install python3-tk
```

**Fedora/RHEL:**
```bash
sudo dnf install python3-tkinter
```

**macOS:**
```bash
brew install python-tk
```

**Windows:**
tkinter ist bereits in der Standard-Python-Installation enthalten.

### 5. FFmpeg installieren

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install ffmpeg
```

**macOS:**
```bash
brew install ffmpeg
```

**Windows:**
```powershell
# Mit winget (empfohlen)
winget install --id=Gyan.FFmpeg -e

# Oder manueller Download
# Lade FFmpeg von https://ffmpeg.org/download.html herunter
# FÃ¼ge FFmpeg zum PATH hinzu
```

### 6. API-Keys konfigurieren

#### FÃ¼r API-Modus:

**OpenAI API Key (erforderlich):**
1. Erstelle einen Account bei [OpenAI](https://platform.openai.com/)
2. Generiere einen API Key unter [API Keys](https://platform.openai.com/api-keys)

**Hugging Face Token (erforderlich):**
1. Erstelle einen Account bei [Hugging Face](https://huggingface.co/)
2. Akzeptiere die Nutzungsbedingungen fÃ¼r [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
3. Generiere einen Token unter [Settings > Access Tokens](https://huggingface.co/settings/tokens)

#### FÃ¼r Lokal-Modus:

**Hugging Face Token (erforderlich):**
- Nur fÃ¼r Pyannote Speaker Diarization erforderlich
- Kein OpenAI API Key nÃ¶tig!

**Umgebungsvariablen setzen:**

**Linux/macOS:**
```bash
export OPENAI_API_KEY='your-openai-api-key-here'
export HF_TOKEN='your-huggingface-token-here'
```

**Windows (PowerShell):**
```powershell
$env:OPENAI_API_KEY='your-openai-api-key-here'
$env:HF_TOKEN='your-huggingface-token-here'
```

**Windows (CMD):**
```cmd
set OPENAI_API_KEY=your-openai-api-key-here
set HF_TOKEN=your-huggingface-token-here
```

Oder erstelle eine `.env` Datei:
```bash
OPENAI_API_KEY=your-openai-api-key-here
HF_TOKEN=your-huggingface-token-here
```

---

## Whisper Modi: API vs. Lokal

InterviewForge unterstÃ¼tzt zwei Transkriptions-Modi:

### ğŸŒ API-Modus (empfohlen fÃ¼r beste QualitÃ¤t)
- **Vorteile:**
  - âœ… Beste TranskriptionsqualitÃ¤t
  - âœ… Bessere Performance bei Hintergrundmusik
  - âœ… Geringere Hardware-Anforderungen
  - âœ… Keine Modell-Downloads erforderlich
- **Nachteile:**
  - âŒ OpenAI API Key erforderlich (kostenpflichtig)
  - âŒ Audio-Daten werden an OpenAI gesendet
  - âŒ Internet-Verbindung erforderlich
  - âŒ Dateilimit: 25 MB

### ğŸ’» Lokal-Modus (Datenschutz)
- **Vorteile:**
  - âœ… **Volle Datenschutz-Kontrolle** (keine Daten verlassen deinen Computer)
  - âœ… Kein OpenAI API Key erforderlich (kostenlos)
  - âœ… Offline-Nutzung mÃ¶glich
  - âœ… Keine DateigrÃ¶ÃŸen-Limits
- **Nachteile:**
  - âŒ HÃ¶here Hardware-Anforderungen (GPU empfohlen)
  - âŒ LÃ¤ngere Verarbeitungszeit
  - âŒ Modell-Download erforderlich (~3GB fÃ¼r large)
  - âŒ Evtl. niedrigere QualitÃ¤t bei komplexen Audios

### ğŸ”„ Auto-Modus (Standard)
- Nutzt API-Modus wenn `OPENAI_API_KEY` gesetzt ist
- FÃ¤llt automatisch auf Lokal-Modus zurÃ¼ck wenn kein API-Key vorhanden

**Empfehlung:**
- **Wissenschaftliche Interviews mit sensiblen Daten:** Lokal-Modus
- **Ã–ffentliche Daten / beste QualitÃ¤t:** API-Modus

---

## Verwendung

### ğŸ–¥ï¸ Grafische BenutzeroberflÃ¤che (GUI) - Empfohlen fÃ¼r Einsteiger

InterviewForge verfÃ¼gt Ã¼ber eine benutzerfreundliche grafische OberflÃ¤che fÃ¼r einfache Bedienung!

**Start der GUI:**

**Linux/macOS:**
```bash
./start_gui.sh
```

**Windows (Doppelklick):**
- `start_gui.bat` (CMD)
- `start_gui.ps1` (PowerShell)

**Oder direkt mit Python:**
```bash
python interviewforge_gui.py
```

**GUI Features:**
- ğŸ“ Einfache Ordnerauswahl per Durchsuchen-Button
- âš™ï¸ Alle Einstellungen an einem Ort:
  - Whisper-Modus (Auto/API/Lokal)
  - Sprecheranzahl
  - ModellgrÃ¶ÃŸe fÃ¼r lokalen Modus
- ğŸ“„ **Ausgabeformate wÃ¤hlbar:**
  - TXT (Kruse-Notation)
  - Markdown (.md)
  - CSV (fÃ¼r Excel/Analyse)
  - HTML (fÃ¼r Browser/PrÃ¤sentation)
- ğŸ”‘ API-Keys direkt eingeben (mit Anzeigen/Verstecken)
- ğŸ“Š Live-Fortschrittsanzeige mit farbigem Log
- ğŸ¯ Start/Stop-Buttons
- ğŸ“‚ Direkter Zugriff auf Output-Ordner

**Screenshot-Ãœbersicht:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ™ï¸ InterviewForge                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“ Eingabe                                   â”‚
â”‚   Audio-Ordner: [C:\audio] [Durchsuchen]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âš™ï¸ Einstellungen                             â”‚
â”‚   Whisper-Modus:    [auto â–¼]                â”‚
â”‚   Anzahl Sprecher:  [2]                     â”‚
â”‚   ModellgrÃ¶ÃŸe:      [medium â–¼]              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“„ Ausgabeformate                            â”‚
â”‚   [x] TXT  [x] Markdown  [ ] CSV  [x] HTML  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”‘ API-Keys                                  â”‚
â”‚   OpenAI:     [***********] [Anzeigen]      â”‚
â”‚   HuggingFace:[***********] [Anzeigen]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š Fortschritt                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [00:12] âœ… Optimierung abgeschlossen    â”‚ â”‚
â”‚ â”‚ [00:13] ğŸ¤ Transkribiere Datei 1/5...  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 60%              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [â–¶ï¸ Starten] [â¹ï¸ Stoppen] [ğŸ“ Output Ã¶ffnen]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vorteile der GUI:**
- âœ… Keine Kommandozeilen-Kenntnisse erforderlich
- âœ… Alle Optionen Ã¼bersichtlich an einem Ort
- âœ… Direkte visuelle RÃ¼ckmeldung
- âœ… API-Keys sicher eingeben (mit Passwort-Schutz)
- âœ… Einfacher Zugriff auf Ergebnisse

---

### ğŸ’» Kommandozeile / Terminal

#### Automatische Pipeline

**Linux/macOS:**
```bash
# Auto-Modus (empfohlen)
./run_pipeline.sh /path/to/audio/folder 2 auto

# API-Modus
./run_pipeline.sh /path/to/audio/folder 2 api

# Lokal-Modus (Datenschutz)
./run_pipeline.sh /path/to/audio/folder 2 local
```

**Windows (PowerShell):**
```powershell
# Auto-Modus (empfohlen)
.\run_pipeline.ps1 -InputDir "C:\audio" -Speakers 2 -Mode auto

# Lokal-Modus (Datenschutz)
.\run_pipeline.ps1 -InputDir "C:\audio" -Speakers 2 -Mode local
```

**Windows (CMD):**
```cmd
REM Auto-Modus
run_pipeline.bat "C:\audio" 2 auto

REM Lokal-Modus
run_pipeline.bat "C:\audio" 2 local
```

Die Pipeline fÃ¼hrt automatisch durch:
1. âœ… Audio-Optimierung (FFmpeg)
2. âœ… Transkription mit Whisper
3. âœ… Speaker Diarization
4. âœ… Kruse-Format Export

### Manuelle Verwendung

**Linux/macOS:**
```bash
# API-Modus
python whisper_kruse_diarization.py /path/to/audio/folder \
  --pattern '*.wav' \
  --speakers 2 \
  --mode api

# Lokal-Modus (Datenschutz)
python whisper_kruse_diarization.py /path/to/audio/folder \
  --pattern '*.wav' \
  --speakers 2 \
  --mode local \
  --model-size medium
```

**Windows:**
```powershell
# API-Modus
python whisper_kruse_diarization.py "C:\audio" --pattern "*.wav" --speakers 2 --mode api

# Lokal-Modus
python whisper_kruse_diarization.py "C:\audio" --pattern "*.wav" --speakers 2 --mode local --model-size medium
```

**VerfÃ¼gbare ModellgrÃ¶ÃŸen (lokal):**
- `tiny` - Schnellstes Modell (~1 GB RAM, niedrige QualitÃ¤t)
- `base` - Standard (~1 GB RAM, gute Balance)
- `small` - Bessere QualitÃ¤t (~2 GB RAM)
- `medium` - Hohe QualitÃ¤t (~5 GB RAM, empfohlen)
- `large-v3` - Beste QualitÃ¤t (~10 GB RAM)

### ğŸ“„ Ausgabeformate

InterviewForge kann Transkripte in **4 verschiedenen Formaten** exportieren:

```bash
# Nur TXT (Standard)
python whisper_kruse_diarization.py ./audio --formats txt

# Mehrere Formate gleichzeitig
python whisper_kruse_diarization.py ./audio --formats txt md html

# Alle Formate
python whisper_kruse_diarization.py ./audio --formats all
```

**Format-Ãœbersicht:**

| Format | Datei | Verwendung | Vorteile |
|--------|-------|------------|----------|
| **TXT** | `.txt` | Wissenschaft | Kruse-Notation mit Zeilennummern |
| **Markdown** | `.md` | Dokumentation | GitHub, Obsidian, Notion |
| **CSV** | `.csv` | Datenanalyse | Excel, SPSS, R, Python, Pandas |
| **HTML** | `.html` | PrÃ¤sentation | Browser, responsive, farbcodiert |

**Format-Beispiele:**

**TXT (Kruse-Notation):**
```
  1 [00:01] I: Wie wÃ¼rden Sie Ihre Erfahrungen beschreiben?
  2
  3 [00:05] P1: Also (..) ich kann sagen, dass es _sehr_
  4     interessant war. Besonders die ersten Wochen (.)
  5     waren herausfordernd.
```

**Markdown (.md):**
```markdown
## Transkript

**[00:01] I:** Wie wÃ¼rden Sie Ihre Erfahrungen beschreiben?

**[00:05] P1:** Also (..) ich kann sagen, dass es _sehr_ interessant war.
Besonders die ersten Wochen (.) waren herausfordernd.
```

**CSV:**
```csv
Zeile,Zeitstempel,Start (s),Ende (s),Dauer (s),Sprecher ID,Sprecher Label,Text
1,00:01,1.00,4.50,3.50,SPEAKER_00,I,"Wie wÃ¼rden Sie Ihre Erfahrungen beschreiben?"
2,00:05,5.20,9.80,4.60,SPEAKER_01,P1,"Also ich kann sagen, dass es sehr interessant war."
```
- âœ… Perfekt fÃ¼r statistische Analyse
- âœ… Import in Excel, SPSS, R
- âœ… Zeitstempel in Sekunden fÃ¼r Berechnungen

**HTML:**
- ğŸ¨ Professionelles Design mit CSS
- ğŸŒˆ Farbcodierte Sprecher
- ğŸ“± Responsive Layout (Desktop & Mobile)
- ğŸ–¨ï¸ Druckoptimiert
- ğŸ¯ Direkt im Browser Ã¶ffnen
- âš¡ Keine Software erforderlich

### Audio-Optimierung (empfohlen)

Optimiere deine Audio-Dateien vor der Transkription:

**Einzelne Datei (alle Systeme):**
```bash
ffmpeg -i input.m4a -ar 16000 -ac 1 -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" output_optimized.wav -y
```

**Batch-Verarbeitung (Linux/macOS):**
```bash
for file in *.m4a; do
  ffmpeg -i "$file" \
    -ar 16000 \
    -ac 1 \
    -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" \
    "${file%.m4a}_optimized.wav" -y
done
```

**Batch-Verarbeitung (Windows PowerShell):**
```powershell
Get-ChildItem *.m4a | ForEach-Object {
  ffmpeg -i $_.FullName `
    -ar 16000 `
    -ac 1 `
    -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" `
    "$($_.BaseName)_optimized.wav" -y
}
```

**Batch-Verarbeitung (Windows CMD):**
```cmd
for %%f in (*.m4a) do ffmpeg -i "%%f" -ar 16000 -ac 1 -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" "%%~nf_optimized.wav" -y
```

**Optimierungs-Parameter:**
- `-ar 16000`: Resampling auf 16 kHz
- `-ac 1`: Mono (Stereo â†’ Mono)
- `highpass=f=200`: Entfernt tieffrequente StÃ¶rgerÃ¤usche
- `lowpass=f=3000`: Entfernt hochfrequente StÃ¶rgerÃ¤usche
- `loudnorm=I=-16`: LautstÃ¤rke-Normalisierung (ITU-R BS.1770-4)

### Parameter

| Parameter | Beschreibung | Standard |
|-----------|--------------|----------|
| `input_dir` | Ordner mit Audio-Dateien | `.` (aktueller Ordner) |
| `--pattern` | Glob-Pattern fÃ¼r Dateien | `*.wav` |
| `--speakers` | Anzahl erwarteter Sprecher | `2` |
| `--config` | Pfad zur Config-Datei | `kruse_config.yaml` |
| `--output` | Output-Ordner | `transcripts_whisper_kruse` |

### Beispiele

**Alle WAV-Dateien transkribieren:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --pattern '*.wav' --speakers 2
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --pattern "*.wav" --speakers 2
```

**Optimierte Dateien verarbeiten:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --pattern '*_optimized.wav' --speakers 2
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --pattern "*_optimized.wav" --speakers 2
```

**Eigene Config verwenden:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --config my_config.yaml
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --config my_config.yaml
```

---

## Konfiguration

Die Sprecherzuordnung kann in `kruse_config.yaml` angepasst werden:

```yaml
speakers:
  SPEAKER_00: "I"    # Interviewer
  SPEAKER_01: "P1"   # Person 1
  SPEAKER_02: "P2"   # Person 2
  SPEAKER_03: "P3"   # Person 3
  SPEAKER_04: "P4"   # Person 4
  SPEAKER_05: "P5"   # Person 5

pause_thresholds:
  short: 0.5    # (.) kurze Pause
  medium: 1.0   # (..) mittlere Pause
  long: 2.0     # (3s) lange Pause
```

---

## Kruse-Notationsformat

Die Pipeline generiert Transkripte im wissenschaftlichen **Kruse-Format**:

### Notationssymbole

| Symbol | Bedeutung | Beispiel |
|--------|-----------|----------|
| `(.)` | Kurze Pause (<1s) | "Ja (.) genau" |
| `(..)` | Mittlere Pause (1-2s) | "Hmm (..) ich denke" |
| `(3s)` | Lange Pause (>2s) | "Warte mal (5s) ja" |
| `((lacht))` | Paraverbale Ã„uÃŸerung | "Das ist lustig ((lacht))" |
| `((seufzt))` | Seufzen | "Ach ((seufzt)) das war schwer" |
| `(unv.)` | UnverstÃ¤ndlich | "Ich war (unv.) gestern" |
| `(?)` | Unsicher | "War das gestern(?)" |
| `/` | Abbruch/Unterbrechung | "Ich wollte noch/ nein" |
| `_wort_` | Betonung | "Das ist _sehr_ wichtig" |
| `wo::rt` | Dehnung | "Ja::a genau" |

### Zeitstempel

Format: `[MM:SS]` am Beginn jedes Sprecherwechsels

### Zeilennummerierung

Fortlaufende Nummerierung fÃ¼r wissenschaftliche Zitation (Zeile 1â†’, 2â†’, etc.)

**Zitat-Beispiel:**
> "Das war eine wichtige Erfahrung" (Interview_01, Z. 15)

---

## Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio-Dateien  â”‚
â”‚  (M4A, WAV)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FFmpeg          â”‚
â”‚ Optimierung     â”‚â—„â”€â”€â”€ 16kHz Mono, Filter, Normalisierung
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI Whisper  â”‚
â”‚ API             â”‚â—„â”€â”€â”€ Spracherkennung
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pyannote Audio  â”‚
â”‚ Diarization     â”‚â—„â”€â”€â”€ Sprecherzuordnung (GPU)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kruse-Format    â”‚
â”‚ Kombination     â”‚â—„â”€â”€â”€ Zeitstempel, Pausen, Nummerierung
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TXT-Output     â”‚
â”‚  (Kruse)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hardware-Anforderungen

### Minimal
- CPU: x86_64 (Intel/AMD)
- RAM: 8GB
- Festplatte: 5GB frei
- Internet: Stabile Verbindung fÃ¼r API-Calls

### Empfohlen
- CPU: Mehrkern (4+ Kerne)
- RAM: 16GB+
- GPU: NVIDIA mit 8GB+ VRAM (fÃ¼r schnelle Diarisation)
- Festplatte: SSD mit 10GB+ frei
- Internet: Schnelle Verbindung (fÃ¼r groÃŸe Audio-Dateien)

---

## Troubleshooting

### GUI startet nicht: "ModuleNotFoundError: No module named 'tkinter'"

**Linux:**
```bash
# Ubuntu/Debian
sudo apt install python3-tk

# Fedora/RHEL
sudo dnf install python3-tkinter
```

**macOS:**
```bash
brew install python-tk
```

**Windows:**
tkinter sollte bereits installiert sein. Falls nicht, installiere Python neu von python.org

### "ModuleNotFoundError: No module named 'openai'"
```bash
pip install -r requirements.txt
```

### "CUDA out of memory"
Reduziere die Batch-GrÃ¶ÃŸe oder nutze CPU:
```python
# In whisper_kruse_diarization.py
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=hf_token
)  # Nutzt automatisch GPU wenn verfÃ¼gbar
```

### "OpenAI API Error: Invalid API Key"
PrÃ¼fe deinen API Key:
```bash
echo $OPENAI_API_KEY
# Sollte mit "sk-" beginnen
```

### "pyannote model not found"
1. Akzeptiere die Nutzungsbedingungen: https://huggingface.co/pyannote/speaker-diarization-3.1
2. PrÃ¼fe deinen HF Token:

Linux/macOS:
```bash
echo $HF_TOKEN
```

Windows (PowerShell):
```powershell
echo $env:HF_TOKEN
```

Windows (CMD):
```cmd
echo %HF_TOKEN%
```

### Windows: "Execution Policy" Fehler bei PowerShell
Wenn du den Fehler "cannot be loaded because running scripts is disabled" erhÃ¤ltst:

```powershell
# Execution Policy temporÃ¤r fÃ¼r diese Sitzung Ã¤ndern
Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process

# Oder dauerhaft fÃ¼r den aktuellen User
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Windows: FFmpeg nicht im PATH
Wenn FFmpeg nicht gefunden wird:

1. Installiere mit winget:
```powershell
winget install --id=Gyan.FFmpeg -e
```

2. Oder fÃ¼ge FFmpeg manuell zum PATH hinzu:
   - Lade FFmpeg von https://ffmpeg.org/download.html
   - Entpacke nach `C:\ffmpeg`
   - FÃ¼ge `C:\ffmpeg\bin` zum System PATH hinzu
   - Starte PowerShell/CMD neu

### Lokales Whisper: Modell-Download
Beim ersten Mal nutzen des lokalen Modus werden Modelle heruntergeladen:

**ModellgrÃ¶ÃŸen:**
- `tiny`: ~75 MB
- `base`: ~150 MB
- `small`: ~500 MB
- `medium`: ~1.5 GB
- `large-v3`: ~3 GB

Die Modelle werden in `~/.cache/whisper/` gespeichert (Linux/macOS) oder `%USERPROFILE%\.cache\whisper\` (Windows).

### Lokales Whisper: GPU-UnterstÃ¼tzung
FÃ¼r schnellere lokale Transkription mit NVIDIA GPU:

**Linux:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Windows:**
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

PrÃ¼fe GPU-Support:
```python
python -c "import torch; print(f'CUDA verfÃ¼gbar: {torch.cuda.is_available()}')"
```

---

## Wissenschaftliche Verwendung

### Methodenbeschreibung (Kopiervorlage)

> Die Transkription erfolgte mittels OpenAI Whisper API (Radford et al., 2022)
> und Pyannote Audio 3.1 (Bredin, 2023) im Kruse-Format (Kruse, 2015).
> Die Audio-Dateien wurden zunÃ¤chst auf 16 kHz Mono resampled und durch
> Hoch- (200 Hz) und Tiefpassfilter (3000 Hz) bereinigt sowie auf -16 LUFS
> normalisiert (ITU-R BS.1770-4).

### Zitationen

**OpenAI Whisper:**
```
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022).
Robust Speech Recognition via Large-Scale Weak Supervision.
arXiv preprint arXiv:2212.04356.
```

**Pyannote Audio:**
```
Bredin, H. (2023).
pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe.
Proceedings of Interspeech 2023.
```

**Kruse-Notation:**
```
Kruse, J. (2015).
Qualitative Interviewforschung: Ein integrativer Ansatz (2. Auflage).
Weinheim: Beltz Juventa.
```

---

## Lizenz

MIT License - siehe [LICENSE](LICENSE) Datei

---

## Beitragen

Contributions sind willkommen! Bitte erstelle einen Pull Request oder Ã¶ffne ein Issue.

1. Fork das Repository
2. Erstelle einen Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Committe deine Ã„nderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. Ã–ffne einen Pull Request

---

## Roadmap

- [x] ~~Lokales Whisper-Modell (ohne API)~~ âœ… Implementiert
- [x] ~~GUI (Grafische BenutzeroberflÃ¤che)~~ âœ… Implementiert
- [ ] Support fÃ¼r mehr Transkriptionsformate (GAT2, HIAT)
- [ ] Web-Interface (Browser-basiert)
- [ ] Docker-Container
- [ ] Automatische QualitÃ¤tskontrolle
- [ ] Multi-Sprach-Support
- [ ] Audio-Recorder Integration

---

## Methodische Grundlagen

Die Entwicklung der InterviewForge-Pipeline wurde durch das Open-Source-Projekt [noScribe](https://github.com/kaixxx/noScribe) (DrÃ¶ge, 2024) inspiriert, welches automatisierte Transkription mit Whisper und Speaker Diarization kombiniert.

**InterviewForge stellt jedoch eine eigenstÃ¤ndige Implementierung dar**, die statt lokaler Modelle die **OpenAI Whisper API** nutzt und eine **spezialisierte Kruse-Notation** fÃ¼r qualitative Forschung implementiert.

### Designentscheidungen

Die Verwendung der OpenAI Whisper API anstelle lokaler Modelle wurde aus folgenden GrÃ¼nden gewÃ¤hlt:

- **Bessere Ergebnisse bei Hintergrundmusik**: Die OpenAI API zeigt robustere Performance bei komplexen Audio-Szenarien mit Musik oder UmgebungsgerÃ¤uschen
- **Datenschutz-Kontext**: FÃ¼r die Transkription Ã¶ffentlicher Fernsehsendungen spielen Datenschutzbedenken eine untergeordnete Rolle
- **Keine lokale Infrastruktur**: Keine Notwendigkeit fÃ¼r leistungsstarke lokale Hardware mit GPU

### Literaturverweis

```
DrÃ¶ge, K. (2024). noScribe. AI-powered Audio Transcription
(Version 0.6) [Computer software]. https://github.com/kaixxx/noScribe
```

---

## Danksagungen

- **[noScribe](https://github.com/kaixxx/noScribe)** von Kai DrÃ¶ge fÃ¼r die Inspiration und das Konzept der Kombination von Whisper + Diarization
- [OpenAI Whisper](https://github.com/openai/whisper) fÃ¼r das Spracherkennungs-Modell
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio) fÃ¼r die Sprecherdiarisation
- [FFmpeg](https://ffmpeg.org/) fÃ¼r die Audio-Verarbeitung
- Jan Kruse fÃ¼r die Kruse-Notation in der qualitativen Interviewforschung

---

## Kontakt

**Issues:** [GitHub Issues](https://github.com/yourusername/InterviewForge/issues)
**Discussions:** [GitHub Discussions](https://github.com/yourusername/InterviewForge/discussions)

---

**Entwickelt mit â¤ï¸ fÃ¼r wissenschaftliche Transkription**
