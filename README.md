# InterviewForge

**Forge your qualitative interviews into scientific transcripts**

Automatisierte Audio-Transkription mit Sprecherdiarisation im wissenschaftlichen Kruse-Format

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![OpenAI Whisper](https://img.shields.io/badge/OpenAI-Whisper-green.svg)](https://openai.com/research/whisper)
[![Pyannote Audio](https://img.shields.io/badge/Pyannote-Audio-orange.svg)](https://github.com/pyannote/pyannote-audio)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## Features

- ‚úÖ **Automatische Transkription** mit OpenAI Whisper (API oder lokal)
- ‚úÖ **Datenschutz-Modus** mit lokalem Whisper (kein API-Key n√∂tig)
- ‚úÖ **Sprecherdiarisation** mit Pyannote Audio 3.1
- ‚úÖ **Wissenschaftliches Format** (Kruse-Notation)
- ‚úÖ **Audio-Optimierung** (FFmpeg)
- ‚úÖ **GPU-beschleunigt** (CUDA-Support)
- ‚úÖ **Batch-Verarbeitung** mehrerer Dateien
- ‚úÖ **Konfigurierbar** via YAML

---

## Beispiel-Output

```
================================================================================
Transkript: interview_01.wav
Datum: 04.11.2025
Format: Kruse-Notation (OpenAI Whisper + Pyannote)
================================================================================

LEGENDE:
  I: SPEAKER_00 (Interviewer)
  P1: SPEAKER_01 (Person 1)

SYMBOLE:
  (.): Kurze Pause
  (..): Mittlere Pause
  (3s): Lange Pause
  ((lacht)): Lachen
  (unv.): Unverst√§ndlich

================================================================================

  1 [00:01] I: Wie w√ºrden Sie Ihre Erfahrungen beschreiben?
  2
  3 [00:05] P1: Also (..) ich kann sagen, dass es _sehr_ interessant war.
  4     Besonders die ersten Wochen (.) waren herausfordernd.
  5
  6 [00:15] I: K√∂nnen Sie das genauer erl√§utern?
```

---

## Installation

### 1. Voraussetzungen

- Python 3.10 oder h√∂her
- FFmpeg
- OpenAI API Key
- Hugging Face Account + Token
- (Optional) NVIDIA GPU mit CUDA f√ºr beschleunigte Diarisation

### 2. Repository klonen

```bash
git clone https://github.com/yourusername/InterviewForge.git
cd InterviewForge
```

### 3. Virtual Environment erstellen

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows
```

### 4. Dependencies installieren

**F√ºr API-Modus (empfohlen f√ºr beste Qualit√§t):**
```bash
pip install -r requirements.txt
```

**F√ºr Lokal-Modus (Datenschutz, ohne OpenAI API):**
```bash
pip install -r requirements-local.txt
```

### 5. FFmpeg installieren

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install ffmpeg
```

**macOS:**
```bash
brew install ffmpeg
```

**Windows:**
```powershell
# Mit winget (empfohlen)
winget install --id=Gyan.FFmpeg -e

# Oder manueller Download
# Lade FFmpeg von https://ffmpeg.org/download.html herunter
# F√ºge FFmpeg zum PATH hinzu
```

### 6. API-Keys konfigurieren

#### F√ºr API-Modus:

**OpenAI API Key (erforderlich):**
1. Erstelle einen Account bei [OpenAI](https://platform.openai.com/)
2. Generiere einen API Key unter [API Keys](https://platform.openai.com/api-keys)

**Hugging Face Token (erforderlich):**
1. Erstelle einen Account bei [Hugging Face](https://huggingface.co/)
2. Akzeptiere die Nutzungsbedingungen f√ºr [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
3. Generiere einen Token unter [Settings > Access Tokens](https://huggingface.co/settings/tokens)

#### F√ºr Lokal-Modus:

**Hugging Face Token (erforderlich):**
- Nur f√ºr Pyannote Speaker Diarization erforderlich
- Kein OpenAI API Key n√∂tig!

**Umgebungsvariablen setzen:**

**Linux/macOS:**
```bash
export OPENAI_API_KEY='your-openai-api-key-here'
export HF_TOKEN='your-huggingface-token-here'
```

**Windows (PowerShell):**
```powershell
$env:OPENAI_API_KEY='your-openai-api-key-here'
$env:HF_TOKEN='your-huggingface-token-here'
```

**Windows (CMD):**
```cmd
set OPENAI_API_KEY=your-openai-api-key-here
set HF_TOKEN=your-huggingface-token-here
```

Oder erstelle eine `.env` Datei:
```bash
OPENAI_API_KEY=your-openai-api-key-here
HF_TOKEN=your-huggingface-token-here
```

---

## Whisper Modi: API vs. Lokal

InterviewForge unterst√ºtzt zwei Transkriptions-Modi:

### üåê API-Modus (empfohlen f√ºr beste Qualit√§t)
- **Vorteile:**
  - ‚úÖ Beste Transkriptionsqualit√§t
  - ‚úÖ Bessere Performance bei Hintergrundmusik
  - ‚úÖ Geringere Hardware-Anforderungen
  - ‚úÖ Keine Modell-Downloads erforderlich
- **Nachteile:**
  - ‚ùå OpenAI API Key erforderlich (kostenpflichtig)
  - ‚ùå Audio-Daten werden an OpenAI gesendet
  - ‚ùå Internet-Verbindung erforderlich
  - ‚ùå Dateilimit: 25 MB

### üíª Lokal-Modus (Datenschutz)
- **Vorteile:**
  - ‚úÖ **Volle Datenschutz-Kontrolle** (keine Daten verlassen deinen Computer)
  - ‚úÖ Kein OpenAI API Key erforderlich (kostenlos)
  - ‚úÖ Offline-Nutzung m√∂glich
  - ‚úÖ Keine Dateigr√∂√üen-Limits
- **Nachteile:**
  - ‚ùå H√∂here Hardware-Anforderungen (GPU empfohlen)
  - ‚ùå L√§ngere Verarbeitungszeit
  - ‚ùå Modell-Download erforderlich (~3GB f√ºr large)
  - ‚ùå Evtl. niedrigere Qualit√§t bei komplexen Audios

### üîÑ Auto-Modus (Standard)
- Nutzt API-Modus wenn `OPENAI_API_KEY` gesetzt ist
- F√§llt automatisch auf Lokal-Modus zur√ºck wenn kein API-Key vorhanden

**Empfehlung:**
- **Wissenschaftliche Interviews mit sensiblen Daten:** Lokal-Modus
- **√ñffentliche Daten / beste Qualit√§t:** API-Modus

---

## Verwendung

### Automatische Pipeline (empfohlen)

**Linux/macOS:**
```bash
# Auto-Modus (empfohlen)
./run_pipeline.sh /path/to/audio/folder 2 auto

# API-Modus
./run_pipeline.sh /path/to/audio/folder 2 api

# Lokal-Modus (Datenschutz)
./run_pipeline.sh /path/to/audio/folder 2 local
```

**Windows (PowerShell):**
```powershell
# Auto-Modus (empfohlen)
.\run_pipeline.ps1 -InputDir "C:\audio" -Speakers 2 -Mode auto

# Lokal-Modus (Datenschutz)
.\run_pipeline.ps1 -InputDir "C:\audio" -Speakers 2 -Mode local
```

**Windows (CMD):**
```cmd
REM Auto-Modus
run_pipeline.bat "C:\audio" 2 auto

REM Lokal-Modus
run_pipeline.bat "C:\audio" 2 local
```

Die Pipeline f√ºhrt automatisch durch:
1. ‚úÖ Audio-Optimierung (FFmpeg)
2. ‚úÖ Transkription mit Whisper
3. ‚úÖ Speaker Diarization
4. ‚úÖ Kruse-Format Export

### Manuelle Verwendung

**Linux/macOS:**
```bash
# API-Modus
python whisper_kruse_diarization.py /path/to/audio/folder \
  --pattern '*.wav' \
  --speakers 2 \
  --mode api

# Lokal-Modus (Datenschutz)
python whisper_kruse_diarization.py /path/to/audio/folder \
  --pattern '*.wav' \
  --speakers 2 \
  --mode local \
  --model-size medium
```

**Windows:**
```powershell
# API-Modus
python whisper_kruse_diarization.py "C:\audio" --pattern "*.wav" --speakers 2 --mode api

# Lokal-Modus
python whisper_kruse_diarization.py "C:\audio" --pattern "*.wav" --speakers 2 --mode local --model-size medium
```

**Verf√ºgbare Modellgr√∂√üen (lokal):**
- `tiny` - Schnellstes Modell (~1 GB RAM, niedrige Qualit√§t)
- `base` - Standard (~1 GB RAM, gute Balance)
- `small` - Bessere Qualit√§t (~2 GB RAM)
- `medium` - Hohe Qualit√§t (~5 GB RAM, empfohlen)
- `large-v3` - Beste Qualit√§t (~10 GB RAM)

### Audio-Optimierung (empfohlen)

Optimiere deine Audio-Dateien vor der Transkription:

**Einzelne Datei (alle Systeme):**
```bash
ffmpeg -i input.m4a -ar 16000 -ac 1 -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" output_optimized.wav -y
```

**Batch-Verarbeitung (Linux/macOS):**
```bash
for file in *.m4a; do
  ffmpeg -i "$file" \
    -ar 16000 \
    -ac 1 \
    -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" \
    "${file%.m4a}_optimized.wav" -y
done
```

**Batch-Verarbeitung (Windows PowerShell):**
```powershell
Get-ChildItem *.m4a | ForEach-Object {
  ffmpeg -i $_.FullName `
    -ar 16000 `
    -ac 1 `
    -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" `
    "$($_.BaseName)_optimized.wav" -y
}
```

**Batch-Verarbeitung (Windows CMD):**
```cmd
for %%f in (*.m4a) do ffmpeg -i "%%f" -ar 16000 -ac 1 -af "highpass=f=200,lowpass=f=3000,loudnorm=I=-16" "%%~nf_optimized.wav" -y
```

**Optimierungs-Parameter:**
- `-ar 16000`: Resampling auf 16 kHz
- `-ac 1`: Mono (Stereo ‚Üí Mono)
- `highpass=f=200`: Entfernt tieffrequente St√∂rger√§usche
- `lowpass=f=3000`: Entfernt hochfrequente St√∂rger√§usche
- `loudnorm=I=-16`: Lautst√§rke-Normalisierung (ITU-R BS.1770-4)

### Parameter

| Parameter | Beschreibung | Standard |
|-----------|--------------|----------|
| `input_dir` | Ordner mit Audio-Dateien | `.` (aktueller Ordner) |
| `--pattern` | Glob-Pattern f√ºr Dateien | `*.wav` |
| `--speakers` | Anzahl erwarteter Sprecher | `2` |
| `--config` | Pfad zur Config-Datei | `kruse_config.yaml` |
| `--output` | Output-Ordner | `transcripts_whisper_kruse` |

### Beispiele

**Alle WAV-Dateien transkribieren:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --pattern '*.wav' --speakers 2
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --pattern "*.wav" --speakers 2
```

**Optimierte Dateien verarbeiten:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --pattern '*_optimized.wav' --speakers 2
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --pattern "*_optimized.wav" --speakers 2
```

**Eigene Config verwenden:**

Linux/macOS:
```bash
python whisper_kruse_diarization.py ./audio --config my_config.yaml
```

Windows:
```powershell
python whisper_kruse_diarization.py .\audio --config my_config.yaml
```

---

## Konfiguration

Die Sprecherzuordnung kann in `kruse_config.yaml` angepasst werden:

```yaml
speakers:
  SPEAKER_00: "I"    # Interviewer
  SPEAKER_01: "P1"   # Person 1
  SPEAKER_02: "P2"   # Person 2
  SPEAKER_03: "P3"   # Person 3
  SPEAKER_04: "P4"   # Person 4
  SPEAKER_05: "P5"   # Person 5

pause_thresholds:
  short: 0.5    # (.) kurze Pause
  medium: 1.0   # (..) mittlere Pause
  long: 2.0     # (3s) lange Pause
```

---

## Kruse-Notationsformat

Die Pipeline generiert Transkripte im wissenschaftlichen **Kruse-Format**:

### Notationssymbole

| Symbol | Bedeutung | Beispiel |
|--------|-----------|----------|
| `(.)` | Kurze Pause (<1s) | "Ja (.) genau" |
| `(..)` | Mittlere Pause (1-2s) | "Hmm (..) ich denke" |
| `(3s)` | Lange Pause (>2s) | "Warte mal (5s) ja" |
| `((lacht))` | Paraverbale √Ñu√üerung | "Das ist lustig ((lacht))" |
| `((seufzt))` | Seufzen | "Ach ((seufzt)) das war schwer" |
| `(unv.)` | Unverst√§ndlich | "Ich war (unv.) gestern" |
| `(?)` | Unsicher | "War das gestern(?)" |
| `/` | Abbruch/Unterbrechung | "Ich wollte noch/ nein" |
| `_wort_` | Betonung | "Das ist _sehr_ wichtig" |
| `wo::rt` | Dehnung | "Ja::a genau" |

### Zeitstempel

Format: `[MM:SS]` am Beginn jedes Sprecherwechsels

### Zeilennummerierung

Fortlaufende Nummerierung f√ºr wissenschaftliche Zitation (Zeile 1‚Üí, 2‚Üí, etc.)

**Zitat-Beispiel:**
> "Das war eine wichtige Erfahrung" (Interview_01, Z. 15)

---

## Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Audio-Dateien  ‚îÇ
‚îÇ  (M4A, WAV)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FFmpeg          ‚îÇ
‚îÇ Optimierung     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ 16kHz Mono, Filter, Normalisierung
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ OpenAI Whisper  ‚îÇ
‚îÇ API             ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ Spracherkennung
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pyannote Audio  ‚îÇ
‚îÇ Diarization     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ Sprecherzuordnung (GPU)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kruse-Format    ‚îÇ
‚îÇ Kombination     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ Zeitstempel, Pausen, Nummerierung
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TXT-Output     ‚îÇ
‚îÇ  (Kruse)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Hardware-Anforderungen

### Minimal
- CPU: x86_64 (Intel/AMD)
- RAM: 8GB
- Festplatte: 5GB frei
- Internet: Stabile Verbindung f√ºr API-Calls

### Empfohlen
- CPU: Mehrkern (4+ Kerne)
- RAM: 16GB+
- GPU: NVIDIA mit 8GB+ VRAM (f√ºr schnelle Diarisation)
- Festplatte: SSD mit 10GB+ frei
- Internet: Schnelle Verbindung (f√ºr gro√üe Audio-Dateien)

---

## Troubleshooting

### "ModuleNotFoundError: No module named 'openai'"
```bash
pip install -r requirements.txt
```

### "CUDA out of memory"
Reduziere die Batch-Gr√∂√üe oder nutze CPU:
```python
# In whisper_kruse_diarization.py
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=hf_token
)  # Nutzt automatisch GPU wenn verf√ºgbar
```

### "OpenAI API Error: Invalid API Key"
Pr√ºfe deinen API Key:
```bash
echo $OPENAI_API_KEY
# Sollte mit "sk-" beginnen
```

### "pyannote model not found"
1. Akzeptiere die Nutzungsbedingungen: https://huggingface.co/pyannote/speaker-diarization-3.1
2. Pr√ºfe deinen HF Token:

Linux/macOS:
```bash
echo $HF_TOKEN
```

Windows (PowerShell):
```powershell
echo $env:HF_TOKEN
```

Windows (CMD):
```cmd
echo %HF_TOKEN%
```

### Windows: "Execution Policy" Fehler bei PowerShell
Wenn du den Fehler "cannot be loaded because running scripts is disabled" erh√§ltst:

```powershell
# Execution Policy tempor√§r f√ºr diese Sitzung √§ndern
Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process

# Oder dauerhaft f√ºr den aktuellen User
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Windows: FFmpeg nicht im PATH
Wenn FFmpeg nicht gefunden wird:

1. Installiere mit winget:
```powershell
winget install --id=Gyan.FFmpeg -e
```

2. Oder f√ºge FFmpeg manuell zum PATH hinzu:
   - Lade FFmpeg von https://ffmpeg.org/download.html
   - Entpacke nach `C:\ffmpeg`
   - F√ºge `C:\ffmpeg\bin` zum System PATH hinzu
   - Starte PowerShell/CMD neu

### Lokales Whisper: Modell-Download
Beim ersten Mal nutzen des lokalen Modus werden Modelle heruntergeladen:

**Modellgr√∂√üen:**
- `tiny`: ~75 MB
- `base`: ~150 MB
- `small`: ~500 MB
- `medium`: ~1.5 GB
- `large-v3`: ~3 GB

Die Modelle werden in `~/.cache/whisper/` gespeichert (Linux/macOS) oder `%USERPROFILE%\.cache\whisper\` (Windows).

### Lokales Whisper: GPU-Unterst√ºtzung
F√ºr schnellere lokale Transkription mit NVIDIA GPU:

**Linux:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Windows:**
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Pr√ºfe GPU-Support:
```python
python -c "import torch; print(f'CUDA verf√ºgbar: {torch.cuda.is_available()}')"
```

---

## Wissenschaftliche Verwendung

### Methodenbeschreibung (Kopiervorlage)

> Die Transkription erfolgte mittels OpenAI Whisper API (Radford et al., 2022)
> und Pyannote Audio 3.1 (Bredin, 2023) im Kruse-Format (Kruse, 2015).
> Die Audio-Dateien wurden zun√§chst auf 16 kHz Mono resampled und durch
> Hoch- (200 Hz) und Tiefpassfilter (3000 Hz) bereinigt sowie auf -16 LUFS
> normalisiert (ITU-R BS.1770-4).

### Zitationen

**OpenAI Whisper:**
```
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022).
Robust Speech Recognition via Large-Scale Weak Supervision.
arXiv preprint arXiv:2212.04356.
```

**Pyannote Audio:**
```
Bredin, H. (2023).
pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe.
Proceedings of Interspeech 2023.
```

**Kruse-Notation:**
```
Kruse, J. (2015).
Qualitative Interviewforschung: Ein integrativer Ansatz (2. Auflage).
Weinheim: Beltz Juventa.
```

---

## Lizenz

MIT License - siehe [LICENSE](LICENSE) Datei

---

## Beitragen

Contributions sind willkommen! Bitte erstelle einen Pull Request oder √∂ffne ein Issue.

1. Fork das Repository
2. Erstelle einen Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Committe deine √Ñnderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. √ñffne einen Pull Request

---

## Roadmap

- [ ] Support f√ºr mehr Transkriptionsformate (GAT2, HIAT)
- [ ] Web-Interface
- [ ] Docker-Container
- [ ] Lokales Whisper-Modell (ohne API)
- [ ] Automatische Qualit√§tskontrolle
- [ ] Multi-Sprach-Support

---

## Methodische Grundlagen

Die Entwicklung der InterviewForge-Pipeline wurde durch das Open-Source-Projekt [noScribe](https://github.com/kaixxx/noScribe) (Dr√∂ge, 2024) inspiriert, welches automatisierte Transkription mit Whisper und Speaker Diarization kombiniert.

**InterviewForge stellt jedoch eine eigenst√§ndige Implementierung dar**, die statt lokaler Modelle die **OpenAI Whisper API** nutzt und eine **spezialisierte Kruse-Notation** f√ºr qualitative Forschung implementiert.

### Designentscheidungen

Die Verwendung der OpenAI Whisper API anstelle lokaler Modelle wurde aus folgenden Gr√ºnden gew√§hlt:

- **Bessere Ergebnisse bei Hintergrundmusik**: Die OpenAI API zeigt robustere Performance bei komplexen Audio-Szenarien mit Musik oder Umgebungsger√§uschen
- **Datenschutz-Kontext**: F√ºr die Transkription √∂ffentlicher Fernsehsendungen spielen Datenschutzbedenken eine untergeordnete Rolle
- **Keine lokale Infrastruktur**: Keine Notwendigkeit f√ºr leistungsstarke lokale Hardware mit GPU

### Literaturverweis

```
Dr√∂ge, K. (2024). noScribe. AI-powered Audio Transcription
(Version 0.6) [Computer software]. https://github.com/kaixxx/noScribe
```

---

## Danksagungen

- **[noScribe](https://github.com/kaixxx/noScribe)** von Kai Dr√∂ge f√ºr die Inspiration und das Konzept der Kombination von Whisper + Diarization
- [OpenAI Whisper](https://github.com/openai/whisper) f√ºr das Spracherkennungs-Modell
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio) f√ºr die Sprecherdiarisation
- [FFmpeg](https://ffmpeg.org/) f√ºr die Audio-Verarbeitung
- Jan Kruse f√ºr die Kruse-Notation in der qualitativen Interviewforschung

---

## Kontakt

**Issues:** [GitHub Issues](https://github.com/yourusername/InterviewForge/issues)
**Discussions:** [GitHub Discussions](https://github.com/yourusername/InterviewForge/discussions)

---

**Entwickelt mit ‚ù§Ô∏è f√ºr wissenschaftliche Transkription**
